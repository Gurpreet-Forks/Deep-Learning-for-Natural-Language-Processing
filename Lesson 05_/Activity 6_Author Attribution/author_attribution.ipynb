{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "Author Attribution is a classic problem of NLP which is a part text classification problem. Authorship attribution is a well-studied problem which led to the field of [Stylometry](https://en.wikipedia.org/wiki/Stylometry).  Here, we given a set of documents from certain authors, we train a model to understand the author's style and use this to indentify the author of unknown documents. As with many other NLP problems, it has benefited greatly from the increase in available computer power, data and advanced machine learning techniques.  All of these make authorship attribution a natural candidate for the use of deep learning (DL).  In particular, we can benefit from DL's ability to automatically extract the relevant features for a specific problem.\n",
    "\n",
    "In this lab we will focus on the following:\n",
    "1.  Extract chracter level features from text of each author (to get author's style)\n",
    "2.  Using these features for building a classification model for authorship attribution\n",
    "3.  Applying the model for identifying the author of a set of unknown documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, this problem can be solved in three steps. First is feature extraction. Here, since there is a limited amount of data, we are going to use character as our features instead of words or sentences. If we use words or sentences as our features, we are going to end up with small dataset which might be problematic to train our model.\n",
    "\n",
    "### Features\n",
    "\n",
    "1. A sequence of characters (length of sequence is a hyperparameter)\n",
    "2. An embedding layer for characters (dimensionality of the embedding is a hyperparameter)\n",
    "\n",
    "Embedding layer is a part of our model, but we can definitely consider it as feature extactor, since it encodes the features space into more meaningful semantic space. \n",
    "\n",
    "### Classifier\n",
    "\n",
    "1. Build a classifier using RNN layers and Dense layers. \n",
    "2. Choose an Optimizer, learning rate and train the model with extacted features\n",
    "\n",
    "### Predict\n",
    "\n",
    "1.  Break the entire document to sequences of the same length, as determined by the hyperparameter\n",
    "2.  Retrieve an author prediction for each one of these sequences\n",
    "3.  Determine which author has received more 'votes'.  We will then use this author as our prediction for the entire document.  (Note:  in order to have a clear majority, we need to ensure that the number of sequences is odd).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "We begin by setting up the data pre-processing pipeline.  For each one of the authors, we aggregate all the known papers into a single long text.  We assume that style does not change across the various papers, hence a single text is equivalent to multiple small ones yet it is much easier to deal with programmatically.\n",
    "\n",
    "For each paper of each author we perform the following steps:\n",
    "1. Convert all text into lower-case (ignoring the fact that capitalization may be a stylistic property)\n",
    "2. Converting all newlines and multiple whitespaces into single whitespaces\n",
    "3. Remove any mention of the authors' names, otherwise we risk data leakage (authors names are hamilton and madison)\n",
    "\n",
    "Do the above steps in a function as it is needed for predicting the unknown papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AuthorA text length: 216394\n",
      "AuthorB text length: 230867\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Classes for A/B/Unknown\n",
    "A = 0\n",
    "B = 1\n",
    "UNKNOWN = -1\n",
    "\n",
    "\n",
    "def preprocess_text(file_path):\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        text = ' '.join(lines[1:]).replace(\"\\n\", ' ').replace('  ',' ').lower().replace('hamilton','').replace('madison', '')\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "\n",
    "\n",
    "# Concatenate all the papers known to be written by A/B into a single long text\n",
    "all_authorA, all_authorB = '',''\n",
    "for x in os.listdir('./papers/A/'):\n",
    "    all_authorA += preprocess_text('./papers/A/' + x)\n",
    "\n",
    "for x in os.listdir('./papers/B/'):\n",
    "    all_authorB += preprocess_text('./papers/B/' + x)\n",
    "    \n",
    "# Print lengths of the large texts\n",
    "print(\"AuthorA text length: {}\".format(len(all_authorA)))\n",
    "print(\"AuthorB text length: {}\".format(len(all_authorB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to break the long text for each author into many small sequences.  As described above, we empirically choose a length for the sequence and use it throughout the model's lifecycle.  We get our full dataset by labeling each sequence with its author.\n",
    "\n",
    "To break the long texts into smaller sequences we use the *Tokenizer* class from the Keras framework.  In particular, note that we set it up to tokenize according to *characters* and not words.\n",
    "\n",
    "1. Choose SEQ_LEN hyper parameter, this might have to be changed if the model doesn't fit well to training data. \n",
    "2. Write a function make_subsequences to turn each document into sequences of length SEQ_LEN and give it a correct label.\n",
    "3. Use keras Tokenizer with char_level=True\n",
    "4. fit the tokenizer on all the texts\n",
    "5. Use this tokenizer to convert all texts into sequences using texts_to_sequences()\n",
    "6. Use make_subsequences() to turn these sequences into appropriate shape and length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 52\n",
      "author A sequences: (216365, 30)\n",
      "author B sequences: (230838, 30)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "# Hyperparameter - sequence length to use for the model\n",
    "SEQ_LEN = 30\n",
    "\n",
    "\n",
    "def make_subsequences(long_sequence, label, sequence_length=SEQ_LEN):\n",
    "\n",
    "    len_sequences = len(long_sequence)\n",
    "    X = np.zeros(((len_sequences - sequence_length)+1, sequence_length))\n",
    "    y = np.zeros((X.shape[0], 1))\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i] = long_sequence[i:i+sequence_length]\n",
    "        y[i] = label\n",
    "    return X,y\n",
    "        \n",
    "# We use the Tokenizer class from Keras to convert the long texts into a sequence of characters (not words)\n",
    "\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "\n",
    "# Make sure to fit all characters in texts from both authors\n",
    "tokenizer.fit_on_texts(all_authorA + all_authorB)\n",
    "\n",
    "authorA_long_sequence = tokenizer.texts_to_sequences([all_authorA])[0]\n",
    "authorB_long_sequence = tokenizer.texts_to_sequences([all_authorB])[0]\n",
    "\n",
    "# Convert the long sequences into sequence and label pairs\n",
    "X_authorA, y_authorA = make_subsequences(authorA_long_sequence, A)\n",
    "X_authorB, y_authorB = make_subsequences(authorB_long_sequence, B)\n",
    "\n",
    "# Print sizes of available data\n",
    "print(\"Number of characters: {}\".format(len(tokenizer.word_index)))\n",
    "print('author A sequences: {}'.format(X_authorA.shape))\n",
    "print('author B sequences: {}'.format(X_authorB.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the number of raw characters to the number of labeled sequences for each author.  Deep Learning requires many examples of each input.  The following code calculates the number of total and unique words in the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count:  74349\n",
      "Total number of unique words:  6318\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of unique words in the text\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts([all_authorA, all_authorB])\n",
    "\n",
    "print(\"Total word count: \", len((all_authorA + ' ' + all_authorB).split(' ')))\n",
    "print(\"Total number of unique words: \", len(word_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to create our train, validation sets.  \n",
    "\n",
    "1. Stack x data together and y data together\n",
    "2. use train_test_split to split the dataset into 80% training and 20% validation\n",
    "3. Reshape the data to make sure that they are sequences of correct length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (357762, 30)\n",
      "y_train shape: (357762, 1)\n",
      "X_validate shape: (89441, 30)\n",
      "y_validate shape: (89441, 1)\n"
     ]
    }
   ],
   "source": [
    "# Take equal amounts of sequences from both authors\n",
    "X = np.vstack((X_authorA, X_authorB))\n",
    "y = np.vstack((y_authorA, y_authorB))\n",
    "\n",
    "# Break data into train and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, train_size=0.8)\n",
    "\n",
    "# Data is to be fed into RNN - ensure that the actual data is of size [batch size, sequence length]\n",
    "X_train = X_train.reshape(-1, SEQ_LEN)\n",
    "X_val =  X_val.reshape(-1, SEQ_LEN) \n",
    "\n",
    "# Print the shapes of the train, validation and test sets\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "\n",
    "print(\"X_validate shape: {}\".format(X_val.shape))\n",
    "print(\"y_validate shape: {}\".format(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we construct the model graph and perform the training procedure.\n",
    "\n",
    "1. Create a model using RNN and Dense layers\n",
    "2. Since its a binary classification problem, the output layer should be Dense with sigmoid activation\n",
    "3. Compile the model with optimizer, appropriate loss function and metrics\n",
    "4. Print the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 100)           5300      \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 256)               91392     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 96,949\n",
      "Trainable params: 96,949\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN, Embedding, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adadelta, Adam\n",
    "Embedding_size = 100\n",
    "RNN_size = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index)+1, Embedding_size, input_length=30))\n",
    "model.add(SimpleRNN(RNN_size, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decide upon the batch size, epochs and train the model using training data and validate with vailadation data\n",
    "2. Based on the results, go back to model above, change it if needed ( use more layers, use regularization, dropout, etc., use different optimizer, or a different learning rate, etc.)\n",
    "3. Change Batch size, epochs if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 357762 samples, validate on 89441 samples\n",
      "Epoch 1/20\n",
      "357762/357762 [==============================] - 7s 20us/step - loss: 0.6907 - acc: 0.5298 - val_loss: 0.6846 - val_acc: 0.5528\n",
      "Epoch 2/20\n",
      "357762/357762 [==============================] - 5s 14us/step - loss: 0.6848 - acc: 0.5521 - val_loss: 0.6864 - val_acc: 0.5457\n",
      "Epoch 3/20\n",
      "357762/357762 [==============================] - 5s 14us/step - loss: 0.6832 - acc: 0.5567 - val_loss: 0.6828 - val_acc: 0.5571\n",
      "Epoch 4/20\n",
      "357762/357762 [==============================] - 5s 14us/step - loss: 0.6829 - acc: 0.5556 - val_loss: 0.6819 - val_acc: 0.5604\n",
      "Epoch 5/20\n",
      "357762/357762 [==============================] - 5s 13us/step - loss: 0.6800 - acc: 0.5621 - val_loss: 0.6760 - val_acc: 0.5718\n",
      "Epoch 6/20\n",
      "357762/357762 [==============================] - 5s 14us/step - loss: 0.6713 - acc: 0.5803 - val_loss: 0.6718 - val_acc: 0.5833\n",
      "Epoch 7/20\n",
      "357762/357762 [==============================] - 5s 14us/step - loss: 0.6650 - acc: 0.5936 - val_loss: 0.6491 - val_acc: 0.6165\n",
      "Epoch 8/20\n",
      "357762/357762 [==============================] - 5s 15us/step - loss: 0.6391 - acc: 0.6309 - val_loss: 0.6230 - val_acc: 0.6488\n",
      "Epoch 9/20\n",
      "357762/357762 [==============================] - 6s 17us/step - loss: 0.6113 - acc: 0.6624 - val_loss: 0.6502 - val_acc: 0.6229\n",
      "Epoch 10/20\n",
      "357762/357762 [==============================] - 8s 21us/step - loss: 0.5674 - acc: 0.7026 - val_loss: 0.5382 - val_acc: 0.7256\n",
      "Epoch 11/20\n",
      "357762/357762 [==============================] - 9s 24us/step - loss: 0.4963 - acc: 0.7568 - val_loss: 0.4697 - val_acc: 0.7745\n",
      "Epoch 12/20\n",
      "357762/357762 [==============================] - 13s 36us/step - loss: 0.4178 - acc: 0.8070 - val_loss: 0.4078 - val_acc: 0.8112\n",
      "Epoch 13/20\n",
      "357762/357762 [==============================] - 16s 46us/step - loss: 0.3448 - acc: 0.8483 - val_loss: 0.3798 - val_acc: 0.8328\n",
      "Epoch 14/20\n",
      "357762/357762 [==============================] - 24s 67us/step - loss: 0.2898 - acc: 0.8759 - val_loss: 0.2925 - val_acc: 0.8746\n",
      "Epoch 15/20\n",
      "357762/357762 [==============================] - 24s 68us/step - loss: 0.2364 - acc: 0.9021 - val_loss: 0.2538 - val_acc: 0.8920\n",
      "Epoch 16/20\n",
      "357762/357762 [==============================] - 24s 66us/step - loss: 0.1934 - acc: 0.9225 - val_loss: 0.2153 - val_acc: 0.9104\n",
      "Epoch 17/20\n",
      "357762/357762 [==============================] - 24s 67us/step - loss: 0.1662 - acc: 0.9345 - val_loss: 0.1931 - val_acc: 0.9206\n",
      "Epoch 18/20\n",
      "357762/357762 [==============================] - 24s 67us/step - loss: 0.1400 - acc: 0.9455 - val_loss: 0.1825 - val_acc: 0.9254\n",
      "Epoch 19/20\n",
      "357762/357762 [==============================] - 27s 76us/step - loss: 0.1249 - acc: 0.9520 - val_loss: 0.1666 - val_acc: 0.9329\n",
      "Epoch 20/20\n",
      "357762/357762 [==============================] - 33s 91us/step - loss: 0.1079 - acc: 0.9591 - val_loss: 0.1503 - val_acc: 0.9400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20f3a8d9ef0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Batch_size = 4096\n",
    "Epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=Batch_size, epochs=Epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Model to the Unknown Papers\n",
    "\n",
    "Do this all the papers in the Unknown folder\n",
    "1. preprocess them same way as training set (lower case, removing white lines, etc.)\n",
    "2. use tokenizer and make_subsequences function above to turn them into sequences of required size\n",
    "3. Use the model to predict on these sequences.\n",
    "4. Count the number of sequences assigned to author A and the ones assigned to author B\n",
    "5. Based on the count, pick the author with highest votes/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 1 is predicted to have been written by Author B, 11946 to 8828\n",
      "Paper 2 is predicted to have been written by Author B, 11267 to 8379\n",
      "Paper 3 is predicted to have been written by Author B, 6738 to 6646\n",
      "Paper 4 is predicted to have been written by Author A, 5254 to 4519\n",
      "Paper 5 is predicted to have been written by Author A, 6570 to 5184\n"
     ]
    }
   ],
   "source": [
    "for x in os.listdir('./papers/Unknown/'):\n",
    "    unknown = preprocess_text('./papers/Unknown/' + x)\n",
    "    unknown_long_sequences = tokenizer.texts_to_sequences([unknown])[0]\n",
    "    X_sequences, _ = make_subsequences(unknown_long_sequences, UNKNOWN)\n",
    "    X_sequences = X_sequences.reshape((-1,SEQ_LEN))\n",
    "    \n",
    "    votes_for_authorA = 0\n",
    "    votes_for_authorB = 0\n",
    "    \n",
    "    y = model.predict(X_sequences)\n",
    "    y = y>0.5\n",
    "    votes_for_authorA = np.sum(y==0)\n",
    "    votes_for_authorB = np.sum(y==1)\n",
    "    \n",
    "    \n",
    "    print(\"Paper {} is predicted to have been written by {}, {} to {}\".format(\n",
    "                x.replace('paper_','').replace('.txt',''), \n",
    "                (\"Author A\" if votes_for_authorA > votes_for_authorB else \"Author B\"),\n",
    "                max(votes_for_authorA, votes_for_authorB), min(votes_for_authorA, votes_for_authorB)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this lab, we discussed the problem of authorship attribution.  Finally, we looked at the model internals to get an intuition for how the it encodes stylometric properties.\n",
    "\n",
    "The first two papers are written by author B, and next three papers are written by author A.\n",
    "\n",
    "The model was able capture the style of each author based on the character sequences given to it. This is a hyper parameter which needs to be tuned. So, play with this parameter as a part of feature extarcation stage.\n",
    "\n",
    "Finally, you are able to train a model to solve author attribution.\n",
    "\n",
    "Good luck for your next lessons! Do try this assigment with the layers you learn next and see if there is any improvement in the model."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
